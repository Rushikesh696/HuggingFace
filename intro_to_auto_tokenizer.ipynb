{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bace3f-2c9f-4b0e-8c68-1d720b38af75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16821f5e-d322-44b5-9479-f5d7684c7018",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abefb62e-dfad-4284-8480-446b1e69bcdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ee2649c-ec17-4d59-ba1a-5a98360f9b26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c63a7eb07cf74ad1895525ef43f130cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manisha\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Manisha\\.cache\\huggingface\\hub\\models--google-bert--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4788cbe54d944c7a9d8de9b14695066",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ca489a1e89c44939d7d7b39c03f0345",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b3fc0c57ea140fa853aadd60d329dd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3d39ff0-e04d-47b3-bb3b-44347b7dd010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter sample text to tokenize\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Please enter sample text to tokenize\"\n",
    "print(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ee318a8-72f9-49d3-944c-27223a192cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 3531, 4607, 7099, 3793, 2000, 19204, 4697, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "# tokenize input\n",
    "token = tokenizer(input_text)\n",
    "print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca6148fb-7333-4c08-802e-ddb0c005ccaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] please enter sample text to tokenize [SEP]'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(token['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eacf611f-daa8-48e9-9d13-8dd99f8a29c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(token['token_type_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8276e51e-b5aa-4669-86be-5d43c5dd3c6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0]'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(token['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37203da7-f4db-4db2-9a27-8dec62c40a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 3531, 4607, 7099, 3793, 2000, 19204, 4697, 102]\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(token['input_ids'])\n",
    "print(type(token['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc8e2c8e-c596-4f18-a059-beb8e0c280fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  101  3531  4607  7099  3793  2000 19204  4697   102]]\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "token=tokenizer(input_text, return_tensors='np')\n",
    "print(token['input_ids'])\n",
    "print(type(token['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6dfe753-127b-4281-9b13-6fa957709705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101,  3531,  4607,  7099,  3793,  2000, 19204,  4697,   102]])\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "token=tokenizer(input_text, return_tensors='pt')\n",
    "print(token['input_ids'])\n",
    "print(type(token['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c8d2c3f-918e-456d-b1c6-ff056d0abbb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TensorFlow and JAX classes are deprecated and will be removed in Transformers v5. We recommend migrating to PyTorch classes or pinning your version of Transformers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[  101  3531  4607  7099  3793  2000 19204  4697   102]], shape=(1, 9), dtype=int32)\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n"
     ]
    }
   ],
   "source": [
    "# TensorFlow and JAX classes are deprecated and will be removed in Transformers v5. \n",
    "# We recommend migrating to PyTorch classes or pinning your version of Transformers.\n",
    "# token=tokenizer(input_text, return_tensors='tf')\n",
    "# print(token['input_ids'])\n",
    "# print(type(token['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "946f457e-9e6c-4ac2-b598-22799cc0af67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1ff331027c247208431314c0643a23c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manisha\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Manisha\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44756d056d9c4e949c58a257de468c91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4f8fa24337b4051826879415cdc5acd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1addbb93e094478893ebf32bd34c5fae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2f3d38c9-1a67-4bb7-b6e8-0d4fa42d41c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_max_length  512\n",
      "vocab_size  30522\n",
      "is_fast  True\n",
      "model_input_names  ['input_ids', 'token_type_ids', 'attention_mask']\n",
      "padding_side  right\n",
      "truncation_side  right\n"
     ]
    }
   ],
   "source": [
    "print(\"model_max_length \",tokenizer.model_max_length)\n",
    "print(\"vocab_size \",tokenizer.vocab_size)\n",
    "print(\"is_fast \",tokenizer.is_fast)\n",
    "print(\"model_input_names \",tokenizer.model_input_names)\n",
    "print(\"padding_side \",tokenizer.padding_side)\n",
    "print(\"truncation_side \",tokenizer.truncation_side)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1498878f-ab7a-45ca-861e-4a64a6c786a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] Token: [CLS] ('ID:', 101)\n",
      "[PAD] Token: [PAD] ('ID:', 102)\n",
      "[SEP] Token: [SEP] ('ID:', 0)\n",
      "[UNK] Token: [UNK] ('ID:', 100)\n",
      "[MASK] Token: [MASK] ('ID:', 103)\n"
     ]
    }
   ],
   "source": [
    "# special tokens\n",
    "print('[CLS] Token:', tokenizer.cls_token, ('ID:', tokenizer.cls_token_id))\n",
    "print('[PAD] Token:', tokenizer.pad_token, ('ID:', tokenizer.sep_token_id))\n",
    "print('[SEP] Token:', tokenizer.sep_token, ('ID:', tokenizer.pad_token_id))\n",
    "print('[UNK] Token:', tokenizer.unk_token, ('ID:', tokenizer.unk_token_id))\n",
    "print('[MASK] Token:', tokenizer.mask_token, ('ID:', tokenizer.mask_token_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fb7eb5c9-3d8b-4c3a-902c-3b55f01dee4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c451cf00020046a18a8bb3d6325436a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manisha\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Manisha\\.cache\\huggingface\\hub\\models--roberta-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a732526a9b0d4486b0d84f6f5770fadb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3423a632e7c40d9adcfa9f568b044f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "668d4206038740c193efaaa4ea186c1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f8b81941e8249b9a5cec0dd9ca0150f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "77237c68-fd48-4657-8d52-50e1b281fa47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(default model max length) model_max_length  512\n",
      "vocab_size  50265\n",
      "is_fast  True\n",
      "model_input_names  ['input_ids', 'attention_mask']\n",
      "padding_side  right\n",
      "truncation_side  right\n"
     ]
    }
   ],
   "source": [
    "print(\"(default model max length) model_max_length \",tokenizer.model_max_length)\n",
    "print(\"vocab_size \",tokenizer.vocab_size)\n",
    "print(\"is_fast \",tokenizer.is_fast)\n",
    "print(\"model_input_names \",tokenizer.model_input_names)\n",
    "print(\"padding_side \",tokenizer.padding_side)\n",
    "print(\"truncation_side \",tokenizer.truncation_side)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9649a2d6-6f6e-430a-bef9-f7a3fceefe18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] Token: <s> ('ID:', 0)\n",
      "[PAD] Token: <pad> ('ID:', 2)\n",
      "[SEP] Token: </s> ('ID:', 1)\n",
      "[UNK] Token: <unk> ('ID:', 3)\n",
      "[MASK] Token: <mask> ('ID:', 50264)\n"
     ]
    }
   ],
   "source": [
    "# special tokens\n",
    "print('[CLS] Token:', tokenizer.cls_token, ('ID:', tokenizer.cls_token_id))\n",
    "print('[PAD] Token:', tokenizer.pad_token, ('ID:', tokenizer.sep_token_id))\n",
    "print('[SEP] Token:', tokenizer.sep_token, ('ID:', tokenizer.pad_token_id))\n",
    "print('[UNK] Token:', tokenizer.unk_token, ('ID:', tokenizer.unk_token_id))\n",
    "print('[MASK] Token:', tokenizer.mask_token, ('ID:', tokenizer.mask_token_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "34c86777-7344-4cdb-8362-bb261eec50df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['please', 'enter', 'sample', 'text', 'to', 'token', '##ize']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "input_text = \"Please enter sample text to tokenize\"\n",
    "token=tokenizer.tokenize(input_text)\n",
    "print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d828e735-8b20-460b-9848-ae416b2781fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3531, 4607, 7099, 3793, 2000, 19204, 4697]\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.convert_tokens_to_ids(token)\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6f4e0451-49e4-4058-a0a8-361232846c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 3531, 4607, 7099, 3793, 2000, 19204, 4697, 102]\n"
     ]
    }
   ],
   "source": [
    "# special tokens\n",
    "out = tokenizer.prepare_for_model(input_ids)\n",
    "special_tokens=out['input_ids']\n",
    "print(special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "029bd35c-ce5b-4fd1-ae2b-2c530ec90fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] please enter sample text to tokenize [SEP]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(special_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d9caf06c-493f-4656-a2a1-4ca48df72f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Please', 'Ġenter', 'Ġsample', 'Ġtext', 'Ġto', 'Ġtoken', 'ize']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "input_text = \"Please enter sample text to tokenize\"\n",
    "token=tokenizer.tokenize(input_text)\n",
    "print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8682e1b5-d716-4214-a2b1-c0c24299818d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 6715, 2914, 7728, 2788, 7, 19233, 2072, 2]\n"
     ]
    }
   ],
   "source": [
    "# special tokens\n",
    "out = tokenizer.prepare_for_model(input_ids)\n",
    "special_tokens=out['input_ids']\n",
    "print(special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6b47e916-2824-498a-bd40-04f314b10977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6715, 2914, 7728, 2788, 7, 19233, 2072]\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.convert_tokens_to_ids(token)\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "10ec51d5-4f11-45a8-825c-c88a7fc6cba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>Please enter sample text to tokenize</s>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(special_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "abc25c26-7453-4f13-9602-c9b0c2f81c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6715, 2914, 7728, 2788, 7, 19233, 2072]\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.convert_tokens_to_ids(token)\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a8cacd4f-f1b8-4b5b-a5e2-9f3134645a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>Please enter sample text to tokenize</s>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(special_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c16e8485-9e9d-4fbe-a574-9e2d58eeac9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['But what about second breakfast?',\n",
       " \"Don't think he knows about second breakfast, Pip.\",\n",
       " 'What about elevensies?',\n",
       " 'This is a much longer sentence that will definitely need to be truncated.',\n",
       " 'Another sentence.']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batched_sentences = [\n",
    "    \"But what about second breakfast?\",\n",
    "    \"Don't think he knows about second breakfast, Pip.\",\n",
    "    \"What about elevensies?\",\n",
    "    \"This is a much longer sentence that will definitely need to be truncated.\",\n",
    "    \"Another sentence.\"\n",
    "]\n",
    "batched_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "738d3a61-c650-46cb-b534-92e68f7b8c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(default model max length) model_max_length  512\n"
     ]
    }
   ],
   "source": [
    "print(\"(default model max length) model_max_length \",tokenizer.model_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f8545917-b4da-420e-8538-8fbd52569114",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f027fefd-eaeb-4dcd-a062-10a002db8a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  2021,  2054,  2055,  2117,  6350,  1029,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0],\n",
      "        [  101,  2123,  1005,  1056,  2228,  2002,  4282,  2055,  2117,  6350,\n",
      "          1010, 28315,  1012,   102,     0,     0],\n",
      "        [  101,  2054,  2055,  5408, 14625,  1029,   102,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0],\n",
      "        [  101,  2023,  2003,  1037,  2172,  2936,  6251,  2008,  2097,  5791,\n",
      "          2342,  2000,  2022, 25449,  1012,   102],\n",
      "        [  101,  2178,  6251,  1012,   102,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "tokenizer_inputs = tokenizer(batched_sentences, padding=True, return_tensors='pt')\n",
    "print(tokenizer_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "77697631-e757-405f-ab7a-e4eb8f1fd660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101,  2021,  2054,  2055,  2117,  6350,  1029,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0],\n",
      "        [  101,  2123,  1005,  1056,  2228,  2002,  4282,  2055,  2117,  6350,\n",
      "          1010, 28315,  1012,   102,     0,     0],\n",
      "        [  101,  2054,  2055,  5408, 14625,  1029,   102,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0],\n",
      "        [  101,  2023,  2003,  1037,  2172,  2936,  6251,  2008,  2097,  5791,\n",
      "          2342,  2000,  2022, 25449,  1012,   102],\n",
      "        [  101,  2178,  6251,  1012,   102,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0]])\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer_inputs['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "452c313d-ce64-4fc0-a216-51a166de0215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 16])\n"
     ]
    }
   ],
   "source": [
    "# 5 no of sentences\n",
    "# 17 sequence length\n",
    "print(tokenizer_inputs['input_ids'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "92202310-26c6-45ab-8399-ce84d00f4465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence 1 [CLS] but what about second breakfast? [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "sentence 2 [CLS] don ' t think he knows about second breakfast, pip. [SEP] [PAD] [PAD]\n",
      "sentence 3 [CLS] what about elevensies? [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "sentence 4 [CLS] this is a much longer sentence that will definitely need to be truncated. [SEP]\n",
      "sentence 5 [CLS] another sentence. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "for i, inputs in enumerate(tokenizer_inputs['input_ids']):\n",
    "    print(f\"sentence {i+1}\",tokenizer.decode(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b6ff0c8f-2daf-44a3-b636-8e1d3489d734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An Error occured : Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    tokenizer_inputs = tokenizer(batched_sentences, truncation=True, return_tensors='pt')\n",
    "except Exception as e:\n",
    "    print(f'An Error occured : {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "6e41269e-fc29-4916-8d5d-834ace7fe1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_inputs = tokenizer(batched_sentences, \n",
    "                             padding='max_length',\n",
    "                             max_length=20,\n",
    "                             truncation=True, \n",
    "                             return_tensors='pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "15154616-35c9-479d-90e3-6d5e04fb8682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 tensor([ 101, 2021, 2054, 2055, 2117, 6350, 1029,  102,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0])\n",
      "sentence 1 [CLS] but what about second breakfast? [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "--------------------\n",
      "20 tensor([  101,  2123,  1005,  1056,  2228,  2002,  4282,  2055,  2117,  6350,\n",
      "         1010, 28315,  1012,   102,     0,     0,     0,     0,     0,     0])\n",
      "sentence 2 [CLS] don ' t think he knows about second breakfast, pip. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "--------------------\n",
      "20 tensor([  101,  2054,  2055,  5408, 14625,  1029,   102,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n",
      "sentence 3 [CLS] what about elevensies? [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "--------------------\n",
      "20 tensor([  101,  2023,  2003,  1037,  2172,  2936,  6251,  2008,  2097,  5791,\n",
      "         2342,  2000,  2022, 25449,  1012,   102,     0,     0,     0,     0])\n",
      "sentence 4 [CLS] this is a much longer sentence that will definitely need to be truncated. [SEP] [PAD] [PAD] [PAD] [PAD]\n",
      "--------------------\n",
      "20 tensor([ 101, 2178, 6251, 1012,  102,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0])\n",
      "sentence 5 [CLS] another sentence. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "for i, inputs in enumerate(tokenizer_inputs['input_ids']):\n",
    "    print(len(inputs), inputs)\n",
    "    print(f\"sentence {i+1}\",tokenizer.decode(inputs))\n",
    "    print(\"-\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49577fea-1f00-405c-8d6f-36c1bcce36f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
