{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c47c8c36-af60-42b2-9435-92bbf4e83c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec3e6253-0de4-4849-9f25-1d4696720540",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76a52fb85a8740069a0ea2de48d71203",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manisha\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Manisha\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint = \"bert-base-uncased\"\n",
    "\n",
    "model = AutoModel.from_pretrained(model_checkpoint)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d05fda92-425a-4ea5-9c3b-13fbd95057d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 1996, 6434, 1997, 1996, 2944, 2003,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\"the output of the model is\", return_tensors='pt')\n",
    "\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b07d65c-2f85-4309-8ff1-0b18add4a826",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manisha\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['last_hidden_state', 'pooler_output'])\n"
     ]
    }
   ],
   "source": [
    "output = model(**inputs)\n",
    "\n",
    "print(output.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08aa7c39-4df0-4c7a-ae6b-0cd7b7e4b677",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 768])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a988e4c-af90-4ebc-8456-7628a6aa45e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.pooler_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8a9d376-6ea5-4f02-ab74-0b4b76ef2e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model type :  bert\n",
      "vocabulary size :  30522\n",
      "number of attention heads :  12\n",
      "hidden size(embedding dimension) :  768\n",
      "number of hidden layers :  12\n",
      "max position embedding (max length it can handled) :  512\n"
     ]
    }
   ],
   "source": [
    "print(\"model type : \", model.config.model_type)\n",
    "print(\"vocabulary size : \", model.config.vocab_size)\n",
    "print(\"number of attention heads : \", model.config.num_attention_heads)\n",
    "print(\"hidden size(embedding dimension) : \", model.config.hidden_size)\n",
    "print(\"number of hidden layers : \", model.config.num_hidden_layers)\n",
    "print(\"max position embedding (max length it can handled) : \", model.config.max_position_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2a960a-40b4-4985-9cb7-84762240e72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\"the output of the model is\", truncate=True, padding=True, return_tensors='pt')\n",
    "\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97d73b33-84d0-4691-94ad-7fe2ab45dfc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check whether the model is in training or not default false\n",
    "model.training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "018e9ed2-27ae-44a6-8f13-a5eca3006b21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set the model to evaluation mode for inference\n",
    "model.eval()\n",
    "model.training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa9d86f6-fbed-4489-9b5b-7c88e87ab2c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- model architecture----\n",
      "BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): BertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSdpaSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(\"---- model architecture----\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af9d4bdc-3183-4a79-9e70-a93969e4002c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54f2e4da-0744-4c0a-9bce-1a36652980e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84f269f5e24d4a20866af5ba8017c414",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/929 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manisha\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Manisha\\.cache\\huggingface\\hub\\models--cardiffnlp--twitter-roberta-base-sentiment-latest. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae59844e0b7f4557896b312c47e945a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/501M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61a84c116ea142428b9e4ba59cf83ef4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/501M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "785bdcaf006d4768bea1f56f8509d891",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e915edc5a4b4486382695dbf6aba67e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab3a8daef1624065a72a345e5cde87e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_checkpoint = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1e941527-598b-4346-8b6f-34ff880c05bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    0,   133,  2111,   544,    21,  3668,  6587,     8,   542, 19178,\n",
      "          2650,     4,     2,     1,     1],\n",
      "        [    0, 32054,    41,  3997,    86,    23,     5,  4192,    94,   363,\n",
      "            93,  4940,   966,    24,     2],\n",
      "        [    0, 31133,  2445,    13,   402,  3571,     7,  1369,   452,     4,\n",
      "             2,     1,     1,     1,     1],\n",
      "        [    0,   100,   439,     7,     5,  9367,   452,     2,     1,     1,\n",
      "             1,     1,     1,     1,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "input_texts = [\n",
    "    \"The customer service was absolutely terrible and unhelpful.\",\n",
    "    \"Had an incredible time at the concert last night — totally worth it!\",\n",
    "    \"Still waiting for something exciting to happen today.\",\n",
    "    \"I went to the mall today\"\n",
    "]\n",
    "\n",
    "token_ids = tokenizer(input_texts, padding=True, truncation=True, max_length=15, return_tensors=\"pt\")\n",
    "\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "36ec5808-d8ba-4787-9602-cdc200544e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.4550, -0.5338, -2.3604],\n",
      "        [-2.0390, -1.4224,  3.4214],\n",
      "        [-2.6849, -0.2877,  2.8138],\n",
      "        [-2.9308,  1.4221,  1.3675]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "# Now pass your preprocessed batch of inputs directly to the model. \n",
    "# You just have to unpack the dictionary by adding **\n",
    "\n",
    "output = model(**token_ids)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "393687ca-db5f-4e4e-bdb4-126468475314",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f2e8b48d-1a73-4f58-9e06-03a119185229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9448, 0.0476, 0.0077],\n",
      "        [0.0042, 0.0078, 0.9880],\n",
      "        [0.0039, 0.0429, 0.9532],\n",
      "        [0.0066, 0.5103, 0.4832]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "prediction = nn.functional.softmax(output.logits, dim=-1)\n",
    "\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8303eb3d-5a7b-4daa-a036-9c2219e4179e",
   "metadata": {},
   "source": [
    "### Printing the predicted labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b84d6c6d-1bb9-470d-9b00-cb8e23cc8730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 2, 2, 1])\n",
      "['negative', 'positive', 'positive', 'neutral']\n",
      "\n",
      "The customer service was absolutely terrible and unhelpful.\n",
      "negative\n",
      "Had an incredible time at the concert last night — totally worth it!\n",
      "positive\n",
      "Still waiting for something exciting to happen today.\n",
      "positive\n",
      "I went to the mall today\n",
      "neutral\n"
     ]
    }
   ],
   "source": [
    "# applying softmax activation function on the output of the model\n",
    "# maximum index value \n",
    "predicted_class_idx = prediction.argmax(dim=1)\n",
    "\n",
    "print(predicted_class_idx)\n",
    "\n",
    "# Define the mapping from indices to class labels\n",
    "class_labels = ['negative', 'neutral', 'positive']\n",
    "\n",
    "predicted_labels = [class_labels[idx] for idx in predicted_class_idx]\n",
    "\n",
    "print(predicted_labels)\n",
    "print()\n",
    "for i in range(len(predicted_labels)):\n",
    "    print(input_texts[i])\n",
    "    print(predicted_labels[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edc13a5-217f-4966-b5f8-c9abb24dfa63",
   "metadata": {},
   "source": [
    "### Sentiment Analysis with pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "490715cd-7e81-415a-b974-80dd102eb7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c83f4976-8988-43f3-a214-1f1c5634c243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Manisha\\AppData\\Roaming\\Python\\Python312\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline(task=\"text-classification\",\n",
    "                     model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5d521cdd-e910-4138-9e52-278cb0c939c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'negative', 'score': 0.9447762370109558}]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"The customer service was absolutely terrible and unhelpful.\"\n",
    "\n",
    "classifier(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ef13f592-39d8-4263-8ef1-da81ba08a769",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "eeb1fb57-ea03-429c-9b87-239bc7d02e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint = \"dslim/bert-base-NER\"\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "83f479a8-baec-475d-adbe-7714cdb95903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 15, 9])\n"
     ]
    }
   ],
   "source": [
    "inputs = \"This is Artificial Intelligence workshop\"\n",
    "tokenized = tokenizer(input_texts, padding=True, truncation=True, max_length=15, return_tensors=\"pt\")\n",
    "output = model(**tokenized)\n",
    "print(output.logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b58b6c06-ba77-410d-a35c-8c63a9a42da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "aeb4b53c-cb52-4db6-9aab-a59eefec7abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "xsum_data = load_dataset(\n",
    "    \"EdinburghNLP/xsum\",\n",
    "    revision=\"refs/convert/parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0e14bfd2-17b4-4dd0-874b-0a809125ed2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "xsum_sample = xsum_data['train'].select(range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c67383-36ed-4d81-a251-cc90fa136eec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c3faf057-0a83-41b7-9368-13f1016964fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6e8141-ab8f-4e3b-ad56-adc6ffb373f5",
   "metadata": {},
   "source": [
    "### Machine translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "026a54c0-e8b9-47b6-a056-61a16880763d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e721cbd69ab349dd9fddd94773d50efb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  76%|#######6  | 755M/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://huggingface.co/google/flan-t5-base/resolve/main/model.safetensors: HTTPSConnectionPool(host='cas-bridge.xethub.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "ename": "ConnectionError",
     "evalue": "(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /google/flan-t5-base/resolve/main/model.safetensors (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000029A2D4B2060>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 608232aa-ff01-4b18-8f3a-6b965cdcac9c)')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\urllib3\\response.py:748\u001b[0m, in \u001b[0;36mHTTPResponse._error_catcher\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 748\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m    750\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    751\u001b[0m     \u001b[38;5;66;03m# FIXME: Ideally we'd like to include the url in the ReadTimeoutError but\u001b[39;00m\n\u001b[0;32m    752\u001b[0m     \u001b[38;5;66;03m# there is yet no clean way to get at it from this context.\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\urllib3\\response.py:873\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[1;34m(self, amt, read1)\u001b[0m\n\u001b[0;32m    872\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[1;32m--> 873\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp_read(amt, read1\u001b[38;5;241m=\u001b[39mread1) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    874\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[0;32m    875\u001b[0m         \u001b[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[0;32m    876\u001b[0m         \u001b[38;5;66;03m# Close the connection when no data is returned\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         \u001b[38;5;66;03m# no harm in redundantly calling close.\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\urllib3\\response.py:856\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[1;34m(self, amt, read1)\u001b[0m\n\u001b[0;32m    854\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    855\u001b[0m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[1;32m--> 856\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread(amt) \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\http\\client.py:479\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    478\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[1;32m--> 479\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mread(amt)\n\u001b[0;32m    480\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[0;32m    481\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[0;32m    482\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\socket.py:720\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    719\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 720\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv_into(b)\n\u001b[0;32m    721\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\ssl.py:1251\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1248\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1249\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1250\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(nbytes, buffer)\n\u001b[0;32m   1252\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\ssl.py:1103\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[0;32m   1104\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mTimeoutError\u001b[0m: The read operation timed out",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mReadTimeoutError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\requests\\models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    819\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 820\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    821\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\urllib3\\response.py:1060\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m   1059\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 1060\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(amt\u001b[38;5;241m=\u001b[39mamt, decode_content\u001b[38;5;241m=\u001b[39mdecode_content)\n\u001b[0;32m   1062\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data:\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\urllib3\\response.py:949\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[0;32m    947\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer\u001b[38;5;241m.\u001b[39mget(amt)\n\u001b[1;32m--> 949\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raw_read(amt)\n\u001b[0;32m    951\u001b[0m flush_decoder \u001b[38;5;241m=\u001b[39m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\urllib3\\response.py:872\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[1;34m(self, amt, read1)\u001b[0m\n\u001b[0;32m    870\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m--> 872\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[0;32m    873\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp_read(amt, read1\u001b[38;5;241m=\u001b[39mread1) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\contextlib.py:158\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[1;34m(self, typ, value, traceback)\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen\u001b[38;5;241m.\u001b[39mthrow(value)\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    160\u001b[0m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[0;32m    161\u001b[0m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\urllib3\\response.py:753\u001b[0m, in \u001b[0;36mHTTPResponse._error_catcher\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    750\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    751\u001b[0m     \u001b[38;5;66;03m# FIXME: Ideally we'd like to include the url in the ReadTimeoutError but\u001b[39;00m\n\u001b[0;32m    752\u001b[0m     \u001b[38;5;66;03m# there is yet no clean way to get at it from this context.\u001b[39;00m\n\u001b[1;32m--> 753\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ReadTimeoutError(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRead timed out.\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m BaseSSLError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    756\u001b[0m     \u001b[38;5;66;03m# FIXME: Is there a better way to differentiate between SSLErrors?\u001b[39;00m\n",
      "\u001b[1;31mReadTimeoutError\u001b[0m: HTTPSConnectionPool(host='cas-bridge.xethub.hf.co', port=443): Read timed out.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\file_download.py:496\u001b[0m, in \u001b[0;36mhttp_get\u001b[1;34m(url, temp_file, proxies, resume_size, headers, expected_size, displayed_filename, _nb_retries, _tqdm_bar)\u001b[0m\n\u001b[0;32m    495\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 496\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m r\u001b[38;5;241m.\u001b[39miter_content(chunk_size\u001b[38;5;241m=\u001b[39mconstants\u001b[38;5;241m.\u001b[39mDOWNLOAD_CHUNK_SIZE):\n\u001b[0;32m    497\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m chunk:  \u001b[38;5;66;03m# filter out keep-alive new chunks\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\requests\\models.py:826\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    825\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ReadTimeoutError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 826\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e)\n\u001b[0;32m    827\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mConnectionError\u001b[0m: HTTPSConnectionPool(host='cas-bridge.xethub.hf.co', port=443): Read timed out.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\urllib3\\connection.py:199\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 199\u001b[0m     sock \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39mcreate_connection(\n\u001b[0;32m    200\u001b[0m         (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dns_host, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mport),\n\u001b[0;32m    201\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout,\n\u001b[0;32m    202\u001b[0m         source_address\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_address,\n\u001b[0;32m    203\u001b[0m         socket_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket_options,\n\u001b[0;32m    204\u001b[0m     )\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\urllib3\\util\\connection.py:60\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LocationParseError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhost\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, label empty or too long\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgetaddrinfo(host, port, family, socket\u001b[38;5;241m.\u001b[39mSOCK_STREAM):\n\u001b[0;32m     61\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\socket.py:976\u001b[0m, in \u001b[0;36mgetaddrinfo\u001b[1;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[0;32m    975\u001b[0m addrlist \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 976\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m _socket\u001b[38;5;241m.\u001b[39mgetaddrinfo(host, port, family, \u001b[38;5;28mtype\u001b[39m, proto, flags):\n\u001b[0;32m    977\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n",
      "\u001b[1;31mgaierror\u001b[0m: [Errno 11001] getaddrinfo failed",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mNameResolutionError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[0;32m    790\u001b[0m     conn,\n\u001b[0;32m    791\u001b[0m     method,\n\u001b[0;32m    792\u001b[0m     url,\n\u001b[0;32m    793\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[0;32m    794\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    795\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    796\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    797\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[0;32m    798\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[0;32m    799\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[0;32m    800\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[0;32m    801\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[0;32m    802\u001b[0m )\n\u001b[0;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:490\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    489\u001b[0m         new_e \u001b[38;5;241m=\u001b[39m _wrap_proxy_error(new_e, conn\u001b[38;5;241m.\u001b[39mproxy\u001b[38;5;241m.\u001b[39mscheme)\n\u001b[1;32m--> 490\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m new_e\n\u001b[0;32m    492\u001b[0m \u001b[38;5;66;03m# conn.request() calls http.client.*.request, not the method in\u001b[39;00m\n\u001b[0;32m    493\u001b[0m \u001b[38;5;66;03m# urllib3.request. It also calls makefile (recv) on the socket.\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:466\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    465\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 466\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_conn(conn)\n\u001b[0;32m    467\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:1095\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m   1094\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[1;32m-> 1095\u001b[0m     conn\u001b[38;5;241m.\u001b[39mconnect()\n\u001b[0;32m   1097\u001b[0m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\urllib3\\connection.py:693\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    692\u001b[0m sock: socket\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m|\u001b[39m ssl\u001b[38;5;241m.\u001b[39mSSLSocket\n\u001b[1;32m--> 693\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m sock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new_conn()\n\u001b[0;32m    694\u001b[0m server_hostname: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\urllib3\\connection.py:206\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 206\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NameResolutionError(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost, \u001b[38;5;28mself\u001b[39m, e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mNameResolutionError\u001b[0m: <urllib3.connection.HTTPSConnection object at 0x0000029A2D4B2060>: Failed to resolve 'huggingface.co' ([Errno 11001] getaddrinfo failed)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\requests\\adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[0;32m    668\u001b[0m         method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[0;32m    669\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m    670\u001b[0m         body\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mbody,\n\u001b[0;32m    671\u001b[0m         headers\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m    672\u001b[0m         redirect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    673\u001b[0m         assert_same_host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    674\u001b[0m         preload_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    675\u001b[0m         decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    676\u001b[0m         retries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries,\n\u001b[0;32m    677\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    678\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    679\u001b[0m     )\n\u001b[0;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:843\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    841\u001b[0m     new_e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_e)\n\u001b[1;32m--> 843\u001b[0m retries \u001b[38;5;241m=\u001b[39m retries\u001b[38;5;241m.\u001b[39mincrement(\n\u001b[0;32m    844\u001b[0m     method, url, error\u001b[38;5;241m=\u001b[39mnew_e, _pool\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, _stacktrace\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39mexc_info()[\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m    845\u001b[0m )\n\u001b[0;32m    846\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\urllib3\\util\\retry.py:519\u001b[0m, in \u001b[0;36mRetry.increment\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    518\u001b[0m     reason \u001b[38;5;241m=\u001b[39m error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause)\n\u001b[1;32m--> 519\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mreason\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    521\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n",
      "\u001b[1;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /google/flan-t5-base/resolve/main/model.safetensors (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000029A2D4B2060>: Failed to resolve 'huggingface.co' ([Errno 11001] getaddrinfo failed)\"))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForSeq2SeqLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoogle/flan-t5-base\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoogle/flan-t5-base\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\models\\auto\\auto_factory.py:600\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    598\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mconfig_class \u001b[38;5;241m==\u001b[39m config\u001b[38;5;241m.\u001b[39msub_configs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    599\u001b[0m         config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget_text_config()\n\u001b[1;32m--> 600\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    601\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    602\u001b[0m     )\n\u001b[0;32m    603\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    604\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    605\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    606\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\modeling_utils.py:315\u001b[0m, in \u001b[0;36mrestore_default_torch_dtype.<locals>._wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    313\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[0;32m    314\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    316\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    317\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\modeling_utils.py:4854\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   4844\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   4845\u001b[0m     gguf_file\n\u001b[0;32m   4846\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   4847\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m ((\u001b[38;5;28misinstance\u001b[39m(device_map, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map)\n\u001b[0;32m   4848\u001b[0m ):\n\u001b[0;32m   4849\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   4850\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOne or more modules is configured to be mapped to disk. Disk offload is not supported for models \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4851\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloaded from GGUF files.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4852\u001b[0m     )\n\u001b[1;32m-> 4854\u001b[0m checkpoint_files, sharded_metadata \u001b[38;5;241m=\u001b[39m _get_resolved_checkpoint_files(\n\u001b[0;32m   4855\u001b[0m     pretrained_model_name_or_path\u001b[38;5;241m=\u001b[39mpretrained_model_name_or_path,\n\u001b[0;32m   4856\u001b[0m     subfolder\u001b[38;5;241m=\u001b[39msubfolder,\n\u001b[0;32m   4857\u001b[0m     variant\u001b[38;5;241m=\u001b[39mvariant,\n\u001b[0;32m   4858\u001b[0m     gguf_file\u001b[38;5;241m=\u001b[39mgguf_file,\n\u001b[0;32m   4859\u001b[0m     from_tf\u001b[38;5;241m=\u001b[39mfrom_tf,\n\u001b[0;32m   4860\u001b[0m     from_flax\u001b[38;5;241m=\u001b[39mfrom_flax,\n\u001b[0;32m   4861\u001b[0m     use_safetensors\u001b[38;5;241m=\u001b[39muse_safetensors,\n\u001b[0;32m   4862\u001b[0m     cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[0;32m   4863\u001b[0m     force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[0;32m   4864\u001b[0m     proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[0;32m   4865\u001b[0m     local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m   4866\u001b[0m     token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m   4867\u001b[0m     user_agent\u001b[38;5;241m=\u001b[39muser_agent,\n\u001b[0;32m   4868\u001b[0m     revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[0;32m   4869\u001b[0m     commit_hash\u001b[38;5;241m=\u001b[39mcommit_hash,\n\u001b[0;32m   4870\u001b[0m     is_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_auto_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   4871\u001b[0m     transformers_explicit_filename\u001b[38;5;241m=\u001b[39mtransformers_explicit_filename,\n\u001b[0;32m   4872\u001b[0m )\n\u001b[0;32m   4874\u001b[0m is_sharded \u001b[38;5;241m=\u001b[39m sharded_metadata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   4875\u001b[0m is_quantized \u001b[38;5;241m=\u001b[39m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\modeling_utils.py:1141\u001b[0m, in \u001b[0;36m_get_resolved_checkpoint_files\u001b[1;34m(pretrained_model_name_or_path, subfolder, variant, gguf_file, from_tf, from_flax, use_safetensors, cache_dir, force_download, proxies, local_files_only, token, user_agent, revision, commit_hash, is_remote_code, transformers_explicit_filename)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1127\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m     cached_file_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcache_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m: cache_dir,\n\u001b[0;32m   1130\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforce_download\u001b[39m\u001b[38;5;124m\"\u001b[39m: force_download,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1139\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m: commit_hash,\n\u001b[0;32m   1140\u001b[0m     }\n\u001b[1;32m-> 1141\u001b[0m     resolved_archive_file \u001b[38;5;241m=\u001b[39m cached_file(pretrained_model_name_or_path, filename, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcached_file_kwargs)\n\u001b[0;32m   1143\u001b[0m     \u001b[38;5;66;03m# Since we set _raise_exceptions_for_missing_entries=False, we don't get an exception but a None\u001b[39;00m\n\u001b[0;32m   1144\u001b[0m     \u001b[38;5;66;03m# result when internet is up, the repo and revision exist, but the file does not.\u001b[39;00m\n\u001b[0;32m   1145\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m resolved_archive_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m filename \u001b[38;5;241m==\u001b[39m _add_variant(SAFE_WEIGHTS_NAME, variant):\n\u001b[0;32m   1146\u001b[0m         \u001b[38;5;66;03m# Maybe the checkpoint is sharded, we try to grab the index name in this case.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\utils\\hub.py:318\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, **kwargs)\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcached_file\u001b[39m(\n\u001b[0;32m    261\u001b[0m     path_or_repo_id: Union[\u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike],\n\u001b[0;32m    262\u001b[0m     filename: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m    263\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    264\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m    265\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;124;03m    Tries to locate a file in a local folder and repo, downloads and cache it if necessary.\u001b[39;00m\n\u001b[0;32m    267\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 318\u001b[0m     file \u001b[38;5;241m=\u001b[39m cached_files(path_or_repo_id\u001b[38;5;241m=\u001b[39mpath_or_repo_id, filenames\u001b[38;5;241m=\u001b[39m[filename], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    319\u001b[0m     file \u001b[38;5;241m=\u001b[39m file[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m file\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m file\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\utils\\hub.py:564\u001b[0m, in \u001b[0;36mcached_files\u001b[1;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    561\u001b[0m     \u001b[38;5;66;03m# Any other Exception type should now be re-raised, in order to provide helpful error messages and break the execution flow\u001b[39;00m\n\u001b[0;32m    562\u001b[0m     \u001b[38;5;66;03m# (EntryNotFoundError will be treated outside this block and correctly re-raised if needed)\u001b[39;00m\n\u001b[0;32m    563\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, EntryNotFoundError):\n\u001b[1;32m--> 564\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    566\u001b[0m resolved_files \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    567\u001b[0m     _get_cache_file_to_return(path_or_repo_id, filename, cache_dir, revision) \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m full_filenames\n\u001b[0;32m    568\u001b[0m ]\n\u001b[0;32m    569\u001b[0m \u001b[38;5;66;03m# If there are any missing file and the flag is active, raise\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\utils\\hub.py:476\u001b[0m, in \u001b[0;36mcached_files\u001b[1;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    474\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(full_filenames) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    475\u001b[0m         \u001b[38;5;66;03m# This is slightly better for only 1 file\u001b[39;00m\n\u001b[1;32m--> 476\u001b[0m         hf_hub_download(\n\u001b[0;32m    477\u001b[0m             path_or_repo_id,\n\u001b[0;32m    478\u001b[0m             filenames[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    479\u001b[0m             subfolder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(subfolder) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m subfolder,\n\u001b[0;32m    480\u001b[0m             repo_type\u001b[38;5;241m=\u001b[39mrepo_type,\n\u001b[0;32m    481\u001b[0m             revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[0;32m    482\u001b[0m             cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[0;32m    483\u001b[0m             user_agent\u001b[38;5;241m=\u001b[39muser_agent,\n\u001b[0;32m    484\u001b[0m             force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[0;32m    485\u001b[0m             proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[0;32m    486\u001b[0m             resume_download\u001b[38;5;241m=\u001b[39mresume_download,\n\u001b[0;32m    487\u001b[0m             token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m    488\u001b[0m             local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m    489\u001b[0m         )\n\u001b[0;32m    490\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    491\u001b[0m         snapshot_download(\n\u001b[0;32m    492\u001b[0m             path_or_repo_id,\n\u001b[0;32m    493\u001b[0m             allow_patterns\u001b[38;5;241m=\u001b[39mfull_filenames,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    502\u001b[0m             local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m    503\u001b[0m         )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[0;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\file_download.py:1010\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[0;32m    990\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[0;32m    991\u001b[0m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[0;32m    992\u001b[0m         local_dir\u001b[38;5;241m=\u001b[39mlocal_dir,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1007\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m   1008\u001b[0m     )\n\u001b[0;32m   1009\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1010\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_cache_dir(\n\u001b[0;32m   1011\u001b[0m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[0;32m   1012\u001b[0m         cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[0;32m   1013\u001b[0m         \u001b[38;5;66;03m# File info\u001b[39;00m\n\u001b[0;32m   1014\u001b[0m         repo_id\u001b[38;5;241m=\u001b[39mrepo_id,\n\u001b[0;32m   1015\u001b[0m         filename\u001b[38;5;241m=\u001b[39mfilename,\n\u001b[0;32m   1016\u001b[0m         repo_type\u001b[38;5;241m=\u001b[39mrepo_type,\n\u001b[0;32m   1017\u001b[0m         revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[0;32m   1018\u001b[0m         \u001b[38;5;66;03m# HTTP info\u001b[39;00m\n\u001b[0;32m   1019\u001b[0m         endpoint\u001b[38;5;241m=\u001b[39mendpoint,\n\u001b[0;32m   1020\u001b[0m         etag_timeout\u001b[38;5;241m=\u001b[39metag_timeout,\n\u001b[0;32m   1021\u001b[0m         headers\u001b[38;5;241m=\u001b[39mhf_headers,\n\u001b[0;32m   1022\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[0;32m   1023\u001b[0m         token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m   1024\u001b[0m         \u001b[38;5;66;03m# Additional options\u001b[39;00m\n\u001b[0;32m   1025\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m   1026\u001b[0m         force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[0;32m   1027\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\file_download.py:1171\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[1;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[0;32m   1168\u001b[0m \u001b[38;5;66;03m# Local file doesn't exist or etag isn't a match => retrieve file from remote (or cache)\u001b[39;00m\n\u001b[0;32m   1170\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m WeakFileLock(lock_path):\n\u001b[1;32m-> 1171\u001b[0m     _download_to_tmp_and_move(\n\u001b[0;32m   1172\u001b[0m         incomplete_path\u001b[38;5;241m=\u001b[39mPath(blob_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.incomplete\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1173\u001b[0m         destination_path\u001b[38;5;241m=\u001b[39mPath(blob_path),\n\u001b[0;32m   1174\u001b[0m         url_to_download\u001b[38;5;241m=\u001b[39murl_to_download,\n\u001b[0;32m   1175\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[0;32m   1176\u001b[0m         headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m   1177\u001b[0m         expected_size\u001b[38;5;241m=\u001b[39mexpected_size,\n\u001b[0;32m   1178\u001b[0m         filename\u001b[38;5;241m=\u001b[39mfilename,\n\u001b[0;32m   1179\u001b[0m         force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[0;32m   1180\u001b[0m         etag\u001b[38;5;241m=\u001b[39metag,\n\u001b[0;32m   1181\u001b[0m         xet_file_data\u001b[38;5;241m=\u001b[39mxet_file_data,\n\u001b[0;32m   1182\u001b[0m     )\n\u001b[0;32m   1183\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(pointer_path):\n\u001b[0;32m   1184\u001b[0m         _create_symlink(blob_path, pointer_path, new_blob\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\file_download.py:1738\u001b[0m, in \u001b[0;36m_download_to_tmp_and_move\u001b[1;34m(incomplete_path, destination_path, url_to_download, proxies, headers, expected_size, filename, force_download, etag, xet_file_data)\u001b[0m\n\u001b[0;32m   1731\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m xet_file_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m constants\u001b[38;5;241m.\u001b[39mHF_HUB_DISABLE_XET:\n\u001b[0;32m   1732\u001b[0m             logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[0;32m   1733\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mXet Storage is enabled for this repo, but the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhf_xet\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m package is not installed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1734\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFalling back to regular HTTP download. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1735\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFor better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1736\u001b[0m             )\n\u001b[1;32m-> 1738\u001b[0m         http_get(\n\u001b[0;32m   1739\u001b[0m             url_to_download,\n\u001b[0;32m   1740\u001b[0m             f,\n\u001b[0;32m   1741\u001b[0m             proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[0;32m   1742\u001b[0m             resume_size\u001b[38;5;241m=\u001b[39mresume_size,\n\u001b[0;32m   1743\u001b[0m             headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m   1744\u001b[0m             expected_size\u001b[38;5;241m=\u001b[39mexpected_size,\n\u001b[0;32m   1745\u001b[0m         )\n\u001b[0;32m   1747\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownload complete. Moving file to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdestination_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1748\u001b[0m _chmod_and_move(incomplete_path, destination_path)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\file_download.py:513\u001b[0m, in \u001b[0;36mhttp_get\u001b[1;34m(url, temp_file, proxies, resume_size, headers, expected_size, displayed_filename, _nb_retries, _tqdm_bar)\u001b[0m\n\u001b[0;32m    511\u001b[0m         time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    512\u001b[0m         reset_sessions()  \u001b[38;5;66;03m# In case of SSLError it's best to reset the shared requests.Session objects\u001b[39;00m\n\u001b[1;32m--> 513\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m http_get(\n\u001b[0;32m    514\u001b[0m             url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m    515\u001b[0m             temp_file\u001b[38;5;241m=\u001b[39mtemp_file,\n\u001b[0;32m    516\u001b[0m             proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[0;32m    517\u001b[0m             resume_size\u001b[38;5;241m=\u001b[39mnew_resume_size,\n\u001b[0;32m    518\u001b[0m             headers\u001b[38;5;241m=\u001b[39minitial_headers,\n\u001b[0;32m    519\u001b[0m             expected_size\u001b[38;5;241m=\u001b[39mexpected_size,\n\u001b[0;32m    520\u001b[0m             _nb_retries\u001b[38;5;241m=\u001b[39m_nb_retries \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m    521\u001b[0m             _tqdm_bar\u001b[38;5;241m=\u001b[39m_tqdm_bar,\n\u001b[0;32m    522\u001b[0m         )\n\u001b[0;32m    524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m expected_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m expected_size \u001b[38;5;241m!=\u001b[39m temp_file\u001b[38;5;241m.\u001b[39mtell():\n\u001b[0;32m    525\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    526\u001b[0m         consistency_error_message\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    527\u001b[0m             actual_size\u001b[38;5;241m=\u001b[39mtemp_file\u001b[38;5;241m.\u001b[39mtell(),\n\u001b[0;32m    528\u001b[0m         )\n\u001b[0;32m    529\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\file_download.py:426\u001b[0m, in \u001b[0;36mhttp_get\u001b[1;34m(url, temp_file, proxies, resume_size, headers, expected_size, displayed_filename, _nb_retries, _tqdm_bar)\u001b[0m\n\u001b[0;32m    420\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    421\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    422\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe file is too large to be downloaded using the regular download method. Use `hf_transfer` or `hf_xet` instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    423\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Try `pip install hf_transfer` or `pip install hf_xet`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    424\u001b[0m         )\n\u001b[1;32m--> 426\u001b[0m r \u001b[38;5;241m=\u001b[39m _request_wrapper(\n\u001b[0;32m    427\u001b[0m     method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGET\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39murl, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, proxies\u001b[38;5;241m=\u001b[39mproxies, headers\u001b[38;5;241m=\u001b[39mheaders, timeout\u001b[38;5;241m=\u001b[39mconstants\u001b[38;5;241m.\u001b[39mHF_HUB_DOWNLOAD_TIMEOUT\n\u001b[0;32m    428\u001b[0m )\n\u001b[0;32m    430\u001b[0m hf_raise_for_status(r)\n\u001b[0;32m    431\u001b[0m total: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m _get_file_length_from_http_response(r)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\file_download.py:309\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[1;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[0;32m    306\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[0;32m    308\u001b[0m \u001b[38;5;66;03m# Perform request and return if status_code is not in the retry list.\u001b[39;00m\n\u001b[1;32m--> 309\u001b[0m response \u001b[38;5;241m=\u001b[39m http_backoff(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams, retry_on_exceptions\u001b[38;5;241m=\u001b[39m(), retry_on_status_codes\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m429\u001b[39m,))\n\u001b[0;32m    310\u001b[0m hf_raise_for_status(response)\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\utils\\_http.py:310\u001b[0m, in \u001b[0;36mhttp_backoff\u001b[1;34m(method, url, max_retries, base_wait_time, max_wait_time, retry_on_exceptions, retry_on_status_codes, **kwargs)\u001b[0m\n\u001b[0;32m    307\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mseek(io_obj_initial_pos)\n\u001b[0;32m    309\u001b[0m \u001b[38;5;66;03m# Perform request and return if status_code is not in the retry list.\u001b[39;00m\n\u001b[1;32m--> 310\u001b[0m response \u001b[38;5;241m=\u001b[39m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m retry_on_status_codes:\n\u001b[0;32m    312\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\utils\\_http.py:96\u001b[0m, in \u001b[0;36mUniqueRequestIdAdapter.send\u001b[1;34m(self, request, *args, **kwargs)\u001b[0m\n\u001b[0;32m     94\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSend: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_curlify(request)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mRequestException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     98\u001b[0m     request_id \u001b[38;5;241m=\u001b[39m request\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(X_AMZN_TRACE_ID)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\requests\\adapters.py:700\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    696\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, _SSLError):\n\u001b[0;32m    697\u001b[0m         \u001b[38;5;66;03m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[0;32m    698\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m--> 700\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ClosedPoolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    703\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "\u001b[1;31mConnectionError\u001b[0m: (MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /google/flan-t5-base/resolve/main/model.safetensors (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000029A2D4B2060>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 608232aa-ff01-4b18-8f3a-6b965cdcac9c)')"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25366b79-9af8-4bfb-8136-a768c13b88aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_text = [\"Translate English to German : How are you?\"]\n",
    "\n",
    "model_inputs = tokenizer(inputs_text, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259f9923-bf14-40e9-bcb9-574c573a9379",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(**model_inputs)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f08346-9548-47a8-b2f5-cfc68989e52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce49af5-6ea1-4ff6-9409-c4a94e577488",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a3338963-fc6f-4ae9-bc26-9402d2af6273",
   "metadata": {},
   "source": [
    "### Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c398d9a-af9d-4490-9f06-e0d708c5c2d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "694fdaa3e0cf4c5fa1c241778e4dbb46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manisha\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Manisha\\.cache\\huggingface\\hub\\models--google--pegasus-cnn_dailymail. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ef5d85c29a9402b9497ded643b9fe68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/2.28G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://huggingface.co/google/pegasus-cnn_dailymail/resolve/main/pytorch_model.bin: HTTPSConnectionPool(host='cas-bridge.xethub.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b30c87b8ff8b4fec9fb90027bbd5cfd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   4%|4         | 94.4M/2.28G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/pegasus-cnn_dailymail\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/pegasus-cnn_dailymail\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ac7905-f588-4cad-a217-e48089a5d4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Climate change is a long-term alteration in the Earth’s climate patterns, especially a rise in global temperatures. \n",
    "It has led to extreme weather conditions like floods, droughts, and wildfires. \n",
    "Human activities, particularly the burning of fossil fuels, deforestation, and industrial emissions, are the primary contributors to climate change. \n",
    "The consequences are widespread, impacting ecosystems, biodiversity, and human societies. \n",
    "Global initiatives, including the Paris Agreement, aim to reduce greenhouse gas emissions and promote sustainable practices to slow down the effects of climate change.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2db087e-90b3-4bc2-bb0f-29f844aab59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = tokenizer(text, padding='longest', truncation=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add2bcbb-7c21-48cc-8c49-29296a483cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = model.generate(**tokenized)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572f8929-757c-43bd-9f5d-b2660f089e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoding = tokenizer.decode(summary, skip_special_tokens=False)\n",
    "\n",
    "print(decoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b568bc97-c3e4-42ac-9c0a-4b154c1a35a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoding = tokenizer.decode(summary, skip_special_tokens=True)\n",
    "\n",
    "print(decoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c262486-a93e-4876-8db1-7b87ea882684",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "822d807a-d9f0-4d9d-9b01-59bc7b927996",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2757fa-cf2c-4602-b5ce-165d5cbf481c",
   "metadata": {},
   "source": [
    "Why We Use \"summarize: \" with T5\n",
    "T5 is a multi-task model, so it needs task-specific prefixes to know what task you want.\n",
    "Examples:\n",
    "\n",
    "\"translate English to French: ...\" → for translation\n",
    "\n",
    "\"summarize: ...\" → for summarization\n",
    "\n",
    "\"cola sentence: ...\" → for grammatical acceptability\n",
    "\n",
    "Without the prefix, T5 won’t know you're asking it to summarize — and the output will be garbage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2e5bc0f6-ab90-45b7-a1ae-7295c7c7ffcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['summarize: The full cost of damage in Newton Stewart, one of the areas worst affected, is still being assessed.\\nRepair work is ongoing in Hawick and many roads in Peeblesshire remain badly affected by standing water.\\nTrains on the west coast mainline face disruption due to damage at the Lamington Viaduct.\\nMany businesses and householders were affected by flooding in Newton Stewart after the River Cree overflowed into the town.\\nFirst Minister Nicola Sturgeon visited the area to inspect the damage.\\nThe waters breached a retaining wall, flooding many commercial properties on Victoria Street - the main shopping thoroughfare.\\nJeanette Tate, who owns the Cinnamon Cafe which was badly affected, said she could not fault the multi-agency response once the flood hit.\\nHowever, she said more preventative work could have been carried out to ensure the retaining wall did not fail.\\n\"It is difficult but I do think there is so much publicity for Dumfries and the Nith - and I totally appreciate that - but it is almost like we\\'re neglected or forgotten,\" she said.\\n\"That may not be true but it is perhaps my perspective over the last few days.\\n\"Why were you not ready to help us a bit more when the warning and the alarm alerts had gone out?\"\\nMeanwhile, a flood alert remains in place across the Borders because of the constant rain.\\nPeebles was badly hit by problems, sparking calls to introduce more defences in the area.\\nScottish Borders Council has put a list on its website of the roads worst affected and drivers have been urged not to ignore closure signs.\\nThe Labour Party\\'s deputy Scottish leader Alex Rowley was in Hawick on Monday to see the situation first hand.\\nHe said it was important to get the flood protection plan right but backed calls to speed up the process.\\n\"I was quite taken aback by the amount of damage that has been done,\" he said.\\n\"Obviously it is heart-breaking for people who have been forced out of their homes and the impact on businesses.\"\\nHe said it was important that \"immediate steps\" were taken to protect the areas most vulnerable and a clear timetable put in place for flood prevention plans.\\nHave you been affected by flooding in Dumfries and Galloway or the Borders? Tell us about your experience of the situation and how it was handled. Email us on selkirk.news@bbc.co.uk or dumfries@bbc.co.uk.', 'summarize: A fire alarm went off at the Holiday Inn in Hope Street at about 04:20 BST on Saturday and guests were asked to leave the hotel.\\nAs they gathered outside they saw the two buses, parked side-by-side in the car park, engulfed by flames.\\nOne of the tour groups is from Germany, the other from China and Taiwan. It was their first night in Northern Ireland.\\nThe driver of one of the buses said many of the passengers had left personal belongings on board and these had been destroyed.\\nBoth groups have organised replacement coaches and will begin their tour of the north coast later than they had planned.\\nPolice have appealed for information about the attack.\\nInsp David Gibson said: \"It appears as though the fire started under one of the buses before spreading to the second.\\n\"While the exact cause is still under investigation, it is thought that the fire was started deliberately.\"', 'summarize: Ferrari appeared in a position to challenge until the final laps, when the Mercedes stretched their legs to go half a second clear of the red cars.\\nSebastian Vettel will start third ahead of team-mate Kimi Raikkonen.\\nThe world champion subsequently escaped punishment for reversing in the pit lane, which could have seen him stripped of pole.\\nBut stewards only handed Hamilton a reprimand, after governing body the FIA said \"no clear instruction was given on where he should park\".\\nBelgian Stoffel Vandoorne out-qualified McLaren team-mate Jenson Button on his Formula 1 debut.\\nVandoorne was 12th and Button 14th, complaining of a handling imbalance on his final lap but admitting the newcomer \"did a good job and I didn\\'t\".\\nMercedes were wary of Ferrari\\'s pace before qualifying after Vettel and Raikkonen finished one-two in final practice, and their concerns appeared to be well founded as the red cars mixed it with the silver through most of qualifying.\\nAfter the first runs, Rosberg was ahead, with Vettel and Raikkonen splitting him from Hamilton, who made a mistake at the final corner on his first lap.\\nBut Hamilton saved his best for last, fastest in every sector of his final attempt, to beat Rosberg by just 0.077secs after the German had out-paced him throughout practice and in the first qualifying session.\\nVettel rued a mistake at the final corner on his last lap, but the truth is that with the gap at 0.517secs to Hamilton there was nothing he could have done.\\nThe gap suggests Mercedes are favourites for the race, even if Ferrari can be expected to push them.\\nVettel said: \"Last year we were very strong in the race and I think we are in good shape for tomorrow. We will try to give them a hard time.\"\\nVandoorne\\'s preparations for his grand prix debut were far from ideal - he only found out he was racing on Thursday when FIA doctors declared Fernando Alonso unfit because of a broken rib sustained in his huge crash at the first race of the season in Australia two weeks ago.\\nThe Belgian rookie had to fly overnight from Japan, where he had been testing in the Super Formula car he races there, and arrived in Bahrain only hours before first practice on Friday.\\nHe also had a difficult final practice, missing all but the final quarter of the session because of a water leak.\\nButton was quicker in the first qualifying session, but Vandoorne pipped him by 0.064secs when it mattered.\\nThe 24-year-old said: \"I knew after yesterday I had quite similar pace to Jenson and I knew if I improved a little bit I could maybe challenge him and even out-qualify him and that is what has happened.\\n\"Jenson is a very good benchmark for me because he is a world champion and he is well known to the team so I am very satisfied with the qualifying.\"\\nButton, who was 0.5secs quicker than Vandoorne in the first session, complained of oversteer on his final run in the second: \"Q1 was what I was expecting. Q2 he did a good job and I didn\\'t. Very, very good job. We knew how quick he was.\"\\nThe controversial new elimination qualifying system was retained for this race despite teams voting at the first race in Australia to go back to the 2015 system.\\nFIA president Jean Todt said earlier on Saturday that he \"felt it necessary to give new qualifying one more chance\", adding: \"We live in a world where there is too much over reaction.\"\\nThe system worked on the basis of mixing up the grid a little - Force India\\'s Sergio Perez ended up out of position in 18th place after the team miscalculated the timing of his final run, leaving him not enough time to complete it before the elimination clock timed him out.\\nBut it will come in for more criticism as a result of lack of track action at the end of each session. There were three minutes at the end of the first session with no cars on the circuit, and the end of the second session was a similar damp squib.\\nOnly one car - Nico Hulkenberg\\'s Force India - was out on the track with six minutes to go. The two Williams cars did go out in the final three minutes but were already through to Q3 and so nothing was at stake.\\nThe teams are meeting with Todt and F1 commercial boss Bernie Ecclestone on Sunday at noon local time to decide on what to do with qualifying for the rest of the season.\\nTodt said he was \"optimistic\" they would be able to reach unanimous agreement on a change.\\n\"We should listen to the people watching on TV,\" Rosberg said. \"If they are still unhappy, which I am sure they will be, we should change it.\"\\nRed Bull\\'s Daniel Ricciardo was fifth on the grid, ahead of the Williams cars of Valtteri Bottas and Felipe Massa and Force India\\'s Nico Hulkenberg.\\nRicciardo\\'s team-mate Daniil Kvyat was eliminated during the second session - way below the team\\'s expectation - and the Renault of Brit Jolyon Palmer only managed 19th fastest.\\nGerman Mercedes protege Pascal Wehrlein managed an excellent 16th in the Manor car.\\nBahrain GP qualifying results\\nBahrain GP coverage details', 'summarize: John Edward Bates, formerly of Spalding, Lincolnshire, but now living in London, faces a total of 22 charges, including two counts of indecency with a child.\\nThe 67-year-old is accused of committing the offences between March 1972 and October 1989.\\nMr Bates denies all the charges.\\nGrace Hale, prosecuting, told the jury that the allegations of sexual abuse were made by made by four male complainants and related to when Mr Bates was a scout leader in South Lincolnshire and Cambridgeshire.\\n\"The defendant says nothing of that sort happened between himself and all these individuals. He says they are all fabricating their accounts and telling lies,\" said Mrs Hale.\\nThe prosecutor claimed Mr Bates invited one 15 year old to his home offering him the chance to look at cine films made at scout camps but then showed him pornographic films.\\nShe told the jury that the boy was then sexually abused leaving him confused and frightened.\\nMrs Hale said: \"The complainant\\'s recollection is that on a number of occasions sexual acts would happen with the defendant either in the defendant\\'s car or in his cottage.\"\\nShe told the jury a second boy was taken by Mr Bates for a weekend in London at the age of 13 or 14 and after visiting pubs he was later sexually abused.\\nMrs Hale said two boys from the Spalding group had also made complaints of being sexually abused.\\nThe jury has been told that Mr Bates was in the RAF before serving as a Lincolnshire Police officer between 1976 and 1983.\\nThe trial, which is expected to last two weeks, continues.', 'summarize: Patients and staff were evacuated from Cerahpasa hospital on Wednesday after a man receiving treatment at the clinic threatened to shoot himself and others.\\nOfficers were deployed to negotiate with the man, a young police officer.\\nEarlier reports that the armed man had taken several people hostage proved incorrect.\\nThe chief consultant of Cerahpasa hospital, Zekayi Kutlubay, who was evacuated from the facility, said that there had been \"no hostage crises\", adding that the man was \"alone in the room\".\\nDr Kutlubay said that the man had been receiving psychiatric treatment for the past two years.\\nHe said that the hospital had previously submitted a report stating that the man should not be permitted to carry a gun.\\n\"His firearm was taken away,\" Dr Kutlubay said, adding that the gun in the officer\\'s possession on Wednesday was not his issued firearm.\\nThe incident comes amid tension in Istanbul following several attacks in crowded areas, including the deadly assault on the Reina nightclub on New Year\\'s Eve which left 39 people dead.', 'summarize: Simone Favaro got the crucial try with the last move of the game, following earlier touchdowns by Chris Fusaro, Zander Fagerson and Junior Bulumakau.\\nRynard Landman and Ashton Hewitt got a try in either half for the Dragons.\\nGlasgow showed far superior strength in depth as they took control of a messy match in the second period.\\nHome coach Gregor Townsend gave a debut to powerhouse Fijian-born Wallaby wing Taqele Naiyaravoro, and centre Alex Dunbar returned from long-term injury, while the Dragons gave first starts of the season to wing Aled Brew and hooker Elliot Dee.\\nGlasgow lost hooker Pat McArthur to an early shoulder injury but took advantage of their first pressure when Rory Clegg slotted over a penalty on 12 minutes.\\nIt took 24 minutes for a disjointed game to produce a try as Sarel Pretorius sniped from close range and Landman forced his way over for Jason Tovey to convert - although it was the lock\\'s last contribution as he departed with a chest injury shortly afterwards.\\nGlasgow struck back when Fusaro drove over from a rolling maul on 35 minutes for Clegg to convert.\\nBut the Dragons levelled at 10-10 before half-time when Naiyaravoro was yellow-carded for an aerial tackle on Brew and Tovey slotted the easy goal.\\nThe visitors could not make the most of their one-man advantage after the break as their error count cost them dearly.\\nIt was Glasgow\\'s bench experience that showed when Mike Blair\\'s break led to a short-range score from teenage prop Fagerson, converted by Clegg.\\nDebutant Favaro was the second home player to be sin-binned, on 63 minutes, but again the Warriors made light of it as replacement wing Bulumakau, a recruit from the Army, pounced to deftly hack through a bouncing ball for an opportunist try.\\nThe Dragons got back within striking range with some excellent combined handling putting Hewitt over unopposed after 72 minutes.\\nHowever, Favaro became sinner-turned-saint as he got on the end of another effective rolling maul to earn his side the extra point with the last move of the game, Clegg converting.\\nDragons director of rugby Lyn Jones said: \"We\\'re disappointed to have lost but our performance was a lot better [than against Leinster] and the game could have gone either way.\\n\"Unfortunately too many errors behind the scrum cost us a great deal, though from where we were a fortnight ago in Dublin our workrate and desire was excellent.\\n\"It was simply error count from individuals behind the scrum that cost us field position, it\\'s not rocket science - they were correct in how they played and we had a few errors, that was the difference.\"\\nGlasgow Warriors: Rory Hughes, Taqele Naiyaravoro, Alex Dunbar, Fraser Lyle, Lee Jones, Rory Clegg, Grayson Hart; Alex Allan, Pat MacArthur, Zander Fagerson, Rob Harley (capt), Scott Cummings, Hugh Blake, Chris Fusaro, Adam Ashe.\\nReplacements: Fergus Scott, Jerry Yanuyanutawa, Mike Cusack, Greg Peterson, Simone Favaro, Mike Blair, Gregor Hunter, Junior Bulumakau.\\nDragons: Carl Meyer, Ashton Hewitt, Ross Wardle, Adam Warren, Aled Brew, Jason Tovey, Sarel Pretorius; Boris Stankovich, Elliot Dee, Brok Harris, Nick Crosswell, Rynard Landman (capt), Lewis Evans, Nic Cudd, Ed Jackson.\\nReplacements: Rhys Buckley, Phil Price, Shaun Knight, Matthew Screech, Ollie Griffiths, Luc Jones, Charlie Davies, Nick Scott.', 'summarize: Veronica Vanessa Chango-Alverez, 31, was killed and another man injured when an Audi A3 struck them in Streatham High Road at 05:30 GMT on Saturday.\\nTen minutes before the crash the car was in London Road, Croydon, when a Volkswagen Passat collided with a tree.\\nPolice want to trace Nathan Davis, 27, who they say has links to the Audi. The car was abandoned at the scene.\\nMs Chango-Alverez died from multiple injuries, a post-mortem examination found.\\nNo arrests have been made as yet, police said.\\nMs Chango-Alverez was staying at her mother\\'s home in Streatham High Road.\\nShe was born in Ecuador and had lived in London for 13 years, BBC London reporter Gareth Furby said. At the time of the crash, she was on her way to work in a hotel.\\nThe remains of the bus stop, which was extensively damaged in the crash, have been removed.\\nFlowers have been left at the site in tribute to the victim.\\nA statement from her brother Kevin Raul Chango-Alverez said: \"My family has had its heart torn out, at this Christmas time, we will never be the same again.\\n\"On Friday night we were together as a family with Veronica meeting her newly born nephew and preparing for Christmas.\\n\"I last saw her alive as she left to go to work on Saturday morning, but moments later I was holding her hand as she passed away in the street.\"\\nDescribing the crash as \"horrific\" Det Insp Gordon Wallace, said: \"The family are devastated. The memory of this senseless death will be with them each time they leave their home.\\n\"The driver fled the scene abandoning the grey Audi, which was extensively damaged.\\n\"We are looking to speak to Mr Nathan Davis in relation to this collision.\"\\nThe 51-year-old man injured at the bus stop remains in a critical condition in hospital while the condition of the 29-year-old driver of the Volkswagen is now stable.', 'summarize: Belgian cyclist Demoitie died after a collision with a motorbike during Belgium\\'s Gent-Wevelgem race.\\nThe 25-year-old was hit by the motorbike after several riders came down in a crash as the race passed through northern France.\\n\"The main issues come when cars or motorbikes have to pass the peloton and pass riders,\" Team Sky\\'s Rowe said.\\n\"That is the fundamental issue we\\'re looking into.\\n\"There\\'s a lot of motorbikes in and around the race whether it be cameras for TV, photographers or police motorbikes.\\n\"In total there\\'s around 50 motorbikes that work on each race.\\n\"We\\'ve got a riders union and we\\'re coming together to think of a few ideas, whether we cap a speed limit on how fast they can overtake us.\\n\"Say we put a 10 kilometres per hour limit on it, if we\\'re going 50kph they\\'re only allowed to pass us 60kph or something like that.\"\\nDemoitie, who was riding for the Wanty-Gobert team, was taken to hospital in Lille but died later.\\nThe sport\\'s governing body, the UCI, said it would co-operate with all relevant authorities in an investigation into the incident.\\nThe Professional Cyclists\\' Association (CPA) issued a statement asking what would be done to improve safety.\\nDespite Demoitie\\'s death, attitudes to road racing will stay the same says Rowe, who has been competing in Three Days of De Panne race in Belgium.\\n\"As soon as that element of fear slips into your mind and you start thinking of things that could happen, that\\'s when you\\'re doomed to fail,\" he told BBC Wales Sport.\\n\"If you start thinking about crashes and the consequences and what could potentially happen then you\\'re never going to be at the front of the peloton and you\\'re never going to win any races.\"\\nIn a separate incident, another Belgian cyclist, Daan Myngheer, 22, died in hospital after suffering a heart attack during the first stage of the Criterium International in Corsica.', 'summarize: Gundogan, 26, told BBC Sport he \"can see the finishing line\" after tearing cruciate knee ligaments in December, but will not rush his return.\\nThe German missed the 2014 World Cup following back surgery that kept him out for a year, and sat out Euro 2016 because of a dislocated kneecap.\\nHe said: \"It is heavy mentally to accept that.\"\\nGundogan will not be fit for the start of the Premier League season at Brighton on 12 August but said his recovery time is now being measured in \"weeks\" rather than months.\\nHe told BBC Sport: \"It is really hard always to fall and fight your way back. You feel good and feel ready, then you get the next kick.\\n\"The worst part is behind me now. I want to feel ready when I am fully back. I want to feel safe and confident. I don\\'t mind if it is two weeks or six.\"\\nGundogan made 15 appearances and scored five goals in his debut season for City following his £20m move from Borussia Dortmund.\\nHe is eager to get on the field again and was impressed at the club\\'s 4-1 win over Real Madrid in a pre-season game in Los Angeles on Wednesday.\\nManager Pep Guardiola has made five new signings already this summer and continues to have an interest in Arsenal forward Alexis Sanchez and Monaco\\'s Kylian Mbappe.\\nGundogan said: \"Optimism for the season is big. It is huge, definitely.\\n\"We felt that last year as well but it was a completely new experience for all of us. We know the Premier League a bit more now and can\\'t wait for the season to start.\"\\nCity complete their three-match tour of the United States against Tottenham in Nashville on Saturday.\\nChelsea manager Antonio Conte said earlier this week he did not feel Tottenham were judged by the same standards as his own side, City and Manchester United.\\nSpurs have had the advantage in their recent meetings with City, winning three and drawing one of their last four Premier League games.\\nAnd Gundogan thinks they are a major threat.\\nHe said: \"Tottenham are a great team. They have the style of football. They have young English players. Our experience last season shows it is really tough to beat them.\\n\"They are really uncomfortable to play against.\\n\"I am pretty sure, even if they will not say it loud, the people who know the Premier League know Tottenham are definitely a competitor for the title.\"', 'summarize: The crash happened about 07:20 GMT at the junction of the A127 and Progress Road in Leigh-on-Sea, Essex.\\nThe man, who police said is aged in his 20s, was treated at the scene for a head injury and suspected multiple fractures, the ambulance service said.\\nHe was airlifted to the Royal London Hospital for further treatment.\\nThe Southend-bound carriageway of the A127 was closed for about six hours while police conducted their initial inquiries.\\nA spokeswoman for Essex Police said it was not possible comment to further as this time as the \"investigation is now being conducted by the IPCC\".']\n"
     ]
    }
   ],
   "source": [
    "# Add \"summarize: \" prefix to each document\n",
    "articles = [\"summarize: \" + doc for doc in xsum_sample[\"document\"]]\n",
    "\n",
    "print(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4919f9cc-55a9-4624-bd3c-8e2f6a160e3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>summarize: The full cost of damage in Newton S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>summarize: A fire alarm went off at the Holida...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>summarize: Ferrari appeared in a position to c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>summarize: John Edward Bates, formerly of Spal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>summarize: Patients and staff were evacuated f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>summarize: Simone Favaro got the crucial try w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>summarize: Veronica Vanessa Chango-Alverez, 31...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>summarize: Belgian cyclist Demoitie died after...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>summarize: Gundogan, 26, told BBC Sport he \"ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>summarize: The crash happened about 07:20 GMT ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             prompts\n",
       "0  summarize: The full cost of damage in Newton S...\n",
       "1  summarize: A fire alarm went off at the Holida...\n",
       "2  summarize: Ferrari appeared in a position to c...\n",
       "3  summarize: John Edward Bates, formerly of Spal...\n",
       "4  summarize: Patients and staff were evacuated f...\n",
       "5  summarize: Simone Favaro got the crucial try w...\n",
       "6  summarize: Veronica Vanessa Chango-Alverez, 31...\n",
       "7  summarize: Belgian cyclist Demoitie died after...\n",
       "8  summarize: Gundogan, 26, told BBC Sport he \"ca...\n",
       "9  summarize: The crash happened about 07:20 GMT ..."
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(articles, columns=['prompts'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "50ac86d6-bfd7-4c6b-a3a6-4fe2ea5e2ab8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'summarize: The full cost of damage in Newton Stewart, one of the areas worst affected, is still being assessed.\\nRepair work is ongoing in Hawick and many roads in Peeblesshire remain badly affected by standing water.\\nTrains on the west coast mainline face disruption due to damage at the Lamington Viaduct.\\nMany businesses and householders were affected by flooding in Newton Stewart after the River Cree overflowed into the town.\\nFirst Minister Nicola Sturgeon visited the area to inspect the damage.\\nThe waters breached a retaining wall, flooding many commercial properties on Victoria Street - the main shopping thoroughfare.\\nJeanette Tate, who owns the Cinnamon Cafe which was badly affected, said she could not fault the multi-agency response once the flood hit.\\nHowever, she said more preventative work could have been carried out to ensure the retaining wall did not fail.\\n\"It is difficult but I do think there is so much publicity for Dumfries and the Nith - and I totally appreciate that - but it is almost like we\\'re neglected or forgotten,\" she said.\\n\"That may not be true but it is perhaps my perspective over the last few days.\\n\"Why were you not ready to help us a bit more when the warning and the alarm alerts had gone out?\"\\nMeanwhile, a flood alert remains in place across the Borders because of the constant rain.\\nPeebles was badly hit by problems, sparking calls to introduce more defences in the area.\\nScottish Borders Council has put a list on its website of the roads worst affected and drivers have been urged not to ignore closure signs.\\nThe Labour Party\\'s deputy Scottish leader Alex Rowley was in Hawick on Monday to see the situation first hand.\\nHe said it was important to get the flood protection plan right but backed calls to speed up the process.\\n\"I was quite taken aback by the amount of damage that has been done,\" he said.\\n\"Obviously it is heart-breaking for people who have been forced out of their homes and the impact on businesses.\"\\nHe said it was important that \"immediate steps\" were taken to protect the areas most vulnerable and a clear timetable put in place for flood prevention plans.\\nHave you been affected by flooding in Dumfries and Galloway or the Borders? Tell us about your experience of the situation and how it was handled. Email us on selkirk.news@bbc.co.uk or dumfries@bbc.co.uk.'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['prompts'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b5654394-793e-40d6-b05d-9a45109988bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[21603,    10,    37,  ...,     0,     0,     0],\n",
      "        [21603,    10,    71,  ...,     0,     0,     0],\n",
      "        [21603,    10, 21945,  ..., 18002,    21,     1],\n",
      "        ...,\n",
      "        [21603,    10, 21768,  ...,     0,     0,     0],\n",
      "        [21603,    10,  9982,  ...,     0,     0,     0],\n",
      "        [21603,    10,    37,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(articles, padding=True, truncation=True, max_length=1024, return_tensors='pt')\n",
    "\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "873859ed-ca7a-4dec-853e-9547d89b0710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,     8,   423,   583,    13,  1783,    16, 20126, 16496,    19,\n",
      "           341,   271, 14841,     3,     5,   186,  7540,    16,   158,    15,\n",
      "          2296,     7,  5718,  2367, 14621,  4161,    57,  4125,   387,     3,\n",
      "             5,     3,     9,  8347,  5685,  3048,    16,   286,   640,     8],\n",
      "        [    0,  1472,  6196,   877,   326,    44,     8,  9108,    86,    29,\n",
      "            16,  6000,  1887,    30,  1856,     3,     5,  2554,   130,  1380,\n",
      "            12,  1175,     8,  1595,     3,     5,    80,    13,     8,   192,\n",
      "         14264,    19,    45, 13692,    63,     6,     8,   119,    45, 20576],\n",
      "        [    0,     3,   849,  2239,     7,   163, 14014,     3,    60,  8234,\n",
      "           232,   227,     3, 19585,   643,   845,   150,  8033,    47,   787,\n",
      "            30,   213,     3,    88,   225,  2447,     3,     5,     3,   849,\n",
      "          2239,     7,   497,     3,    31,    29,    32,   964,  8033,    47],\n",
      "        [    0,     8,     3,  3708,    18,  1201,    18,  1490,    19, 11970,\n",
      "            13,     3, 25345,     8, 10883,  2319,   344,  1332, 16583,    11,\n",
      "          1797,  9975,     3,     5,     3,    88,   177,   725,    66,     8,\n",
      "          3991,     6,   379,   192, 12052,    13,    16,   221,    75,  4392],\n",
      "        [    0,     3,     9,   388,  4281,  1058,    44,     8,  5998, 16026,\n",
      "            12,  4279,  2448,    11,   717,     3,     5,     3,     9,  1021,\n",
      "          2095,  5502,    47, 29766,    26,    45,     8,  2833,     3,     5,\n",
      "             8,  5415,   639, 18905,  7012,    16, 20958,   826,   633,  6032],\n",
      "        [    0,     3, 26705,  4463,     7,   989,  1891,     3,     9,  5695,\n",
      "            12,   579,  1840,     3,  3108,  2067,  1824,   400,  1823,    23,\n",
      "            63,  2551,  1967,    32,     3,     5,     8, 14580,     7,  1891,\n",
      "           166,  3511,    13,     8,   774,    12,     3,  3108,     3,     9],\n",
      "        [    0,     3, 31873, 30003, 24187,    32,    18,   188,    40,   624,\n",
      "           457,     6, 12074,    47,  4792,    11,   430,   388,  7532,    16,\n",
      "             8,  8420,     3,     5,  2095,   241,    12,  8320, 18050,  8688,\n",
      "             6, 14141,   113,    65,  2416,    12,     8,  9835,     3,     5],\n",
      "        [    0,     8,   944,    18,  1201,    18,  1490,    47,  1560,    57,\n",
      "             3,     9,  2340, 15214,   383,     8, 21689,    18,  1326,  4911,\n",
      "           397,    51,  1964,     3,     5,     8,  1964,  2804,   190,  8390,\n",
      "          2515,   663,     3,     5,     8,  2600,    31,     7,     3, 19585],\n",
      "        [    0,  4740, 10169,   152,   845,     3,    88,    54,   217,     8,\n",
      "          8619,   689,   227,     3, 30846,     3, 22052,   342,  6476, 27588,\n",
      "             7,    16,  1882,     3,     5,     8, 13692,  4785,     8,  1412,\n",
      "           296,  4119,   227,   223,  3730,    24,  2697,   376,    91,    21],\n",
      "        [    0,     8,  8420,  2817,    81, 10668,    10,  1755, 22866,    44,\n",
      "             8, 23704,    13,     8,    71, 22367,    11, 24583,  2409,    16,\n",
      "            90,  9031,    18,   106,    18,   134,    15,     9,     6, 25223,\n",
      "             3,     5,     8,   388,     6,  9742,    16,   112,   460,     7]])\n"
     ]
    }
   ],
   "source": [
    "summary_ids = model.generate(\n",
    "    inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    num_beams=2,\n",
    "    max_length=40,\n",
    "    min_length=0\n",
    ")\n",
    "print(summary_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "de7f6d21-2a0b-4c0f-9d2d-10e4d0bc0cbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the full cost of damage in Newton Stewart is still being assessed. many roads in peeblesshire remain badly affected by standing water. a flood alert remains in place across the'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_summary = tokenizer.batch_decode(summary_ids, skip_special_tokens=True)\n",
    "\n",
    "decoded_df = pd.DataFrame(decoded_summary, columns=['decoded_summary'])\n",
    "decoded_df['decoded_summary'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7150cbfc-80ff-49bf-9d2d-b7079f6ac871",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "453425ad-2a66-48c4-93a5-399aef1690a8",
   "metadata": {},
   "source": [
    "### Question Answering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30c8ed5b-2510-4a44-a553-1e01aed55e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d52e160-16b8-4a8f-b295-ccc19bb7ca9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForQuestionAnswering.from_pretrained(\"distilbert/distilbert-base-cased-distilled-squad\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-cased-distilled-squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51d37144-d6a0-4083-a2e9-2fff0af7e6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "article = \"\"\"\n",
    "Transfer learning is a machine learning technique where a model trained on one task is repurposed on a second, related task. \n",
    "This approach is especially useful when the second task has limited training data. Pretrained models like BERT and GPT have \n",
    "been successfully used in various NLP tasks through transfer learning.\n",
    "\"\"\"\n",
    "\n",
    "question = \"Which machine learning technique allows a model to be reused for a related task?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0170258c-facb-4c91-b001-2791a48c44a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  5979,  3395,  3776,  5531,  3643,   170,  2235,  1106,  1129,\n",
      "          1231, 11031,  1111,   170,  2272,  4579,   136,   102, 23489,  3776,\n",
      "          1110,   170,  3395,  3776,  5531,  1187,   170,  2235,  3972,  1113,\n",
      "          1141,  4579,  1110,  1231,  4093, 13541,  1113,   170,  1248,   117,\n",
      "          2272,  4579,   119,  1188,  3136,  1110,  2108,  5616,  1165,  1103,\n",
      "          1248,  4579,  1144,  2609,  2013,  2233,   119, 11689,  4487,  9044,\n",
      "          3584,  1176,   139,  9637,  1942,  1105, 15175,  1942,  1138,  1151,\n",
      "          4358,  1215,  1107,  1672, 21239,  2101,  8249,  1194,  4036,  3776,\n",
      "           119,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "--------------------------------------------------\n",
      "QuestionAnsweringModelOutput(loss=None, start_logits=tensor([[ -4.5322,  -6.8582,  -6.4891,  -7.9156,  -8.4023,  -9.1701, -10.1140,\n",
      "         -10.0497, -10.4605, -10.5644,  -9.6390, -10.2935, -10.1031, -10.1440,\n",
      "          -9.8867,  -9.7836,  -6.1208,  -8.1209,   8.8356,   0.1020,  -4.3633,\n",
      "          -2.1259,  -1.3720,  -4.8197,  -3.5659,  -6.0996,  -4.3005,  -3.6098,\n",
      "          -4.7157,  -7.1136,  -5.2257,  -6.4598,  -7.4904,  -4.1500,  -7.5916,\n",
      "          -7.3025,  -6.5068,  -4.0352,  -3.8915,  -7.4770,  -4.4467,  -5.8924,\n",
      "          -7.1801,  -4.0116,  -5.8457,  -8.6659,  -7.7259,  -6.7300,  -6.6555,\n",
      "          -5.2149,  -3.1906,  -6.3917,  -8.2544,  -6.4111,  -5.9890,  -6.6335,\n",
      "          -6.7492,  -1.7110,  -7.2784,  -7.2200,  -4.8509,  -9.1611,  -2.6757,\n",
      "          -7.4723,  -6.9284,  -9.4391,  -3.5694,  -6.8025,  -8.7842,  -9.4705,\n",
      "          -7.5634,  -7.5624,  -8.3090,  -6.6472,  -3.9531,  -7.3859,  -6.6884,\n",
      "          -5.6756,  -0.0904,  -4.4666,  -7.5567,  -8.1209]],\n",
      "       grad_fn=<CloneBackward0>), end_logits=tensor([[ -2.7034,  -7.2716,  -8.2188,  -7.0828,  -7.2894, -10.0601, -10.5147,\n",
      "          -9.1844, -10.3778, -10.2259, -10.4326,  -8.5590, -10.0309,  -9.7960,\n",
      "          -9.2361,  -7.3308,  -6.7169,  -8.5154,   5.1790,   9.1584,  -0.9740,\n",
      "          -4.5227,  -4.2875,  -1.6494,   1.0390,  -7.3660,  -7.5428,  -4.4271,\n",
      "          -5.5959,  -6.8042,  -5.1045,  -3.4138,  -7.2137,  -6.5213,  -7.2075,\n",
      "          -3.1004,  -6.2385,  -6.7857,  -2.4358,  -4.1452,  -2.8067,  -0.5707,\n",
      "          -2.5166,  -6.0575,  -5.5996,  -9.3692,  -9.2867,  -6.5559,  -8.7075,\n",
      "          -8.3497,  -3.8223,  -4.0849,  -8.6397,  -6.9851,  -5.3614,  -2.2809,\n",
      "          -3.6719,  -6.4471,  -7.3677,  -4.7749,  -2.4131,  -9.1680,  -5.8037,\n",
      "          -6.8325,  -2.0830,  -8.2480,  -5.9513,  -1.7522,  -8.7648,  -8.8217,\n",
      "          -7.2369,  -6.0684,  -8.3887,  -6.8032,  -6.5475,  -2.4830,  -3.1603,\n",
      "          -7.1024,  -1.5683,  -0.2205,  -3.1115,  -8.5153]],\n",
      "       grad_fn=<CloneBackward0>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(question, article, return_tensors='pt')\n",
    "\n",
    "print(inputs)\n",
    "print('-'*50)\n",
    "\n",
    "output = model(**inputs)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "760ab8c5-881b-4b18-9066-3680fac2f9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_logits=output.start_logits\n",
    "end_logits=output.end_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "84c0e00f-297a-494d-981b-de5babeddb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "start_index = torch.argmax(start_logits) \n",
    "end_index = torch.argmax(end_logits)+1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f64649e2-7c12-4d07-af4b-8febf7cb45e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([23489,  3776])\n"
     ]
    }
   ],
   "source": [
    "# Convert token IDs to answer string\n",
    "answer_tokens = inputs['input_ids'][0][start_index:end_index]\n",
    "print(answer_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "18481b06-a349-46f0-953e-65295d945930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transfer learning\n"
     ]
    }
   ],
   "source": [
    "answer = tokenizer.decode(answer_tokens, skip_special_tokens=True)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726dac9d-b0c5-4322-bec9-e4b0a71ed65d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f3669a-373c-46c8-aa6e-cb9bfc71e30d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36465d4b-f93e-40fd-a878-69044590c317",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2ff2a6-32cc-4cb8-8437-8564818af278",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
